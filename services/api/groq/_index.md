---
title: Groq - API
weight: 2
comments: true
sidebar:
  open: true
---

## ğŸ“‹ æœåŠ¡ä¿¡æ¯

**æä¾›è€…ï¼š** [Groq](/providers/groq)  
**æœåŠ¡ç±»å‹ï¼š** API æœåŠ¡  
**API ç«¯ç‚¹ï¼š** [https://api.groq.com/openai/v1](https://api.groq.com/openai/v1)  
**å…è´¹ç±»å‹ï¼š** æ°¸ä¹…å…è´¹ï¼ˆæœ‰ä½¿ç”¨é™åˆ¶ï¼‰  
**API å…¼å®¹æ€§ï¼š** å®Œå…¨å…¼å®¹ OpenAI API

---

## ğŸ¯ æœåŠ¡ç®€ä»‹

Groq API æä¾›ä¸šç•Œæœ€å¿«çš„ LLM æ¨ç†æœåŠ¡ï¼Œå®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼ï¼Œè®©æ‚¨åªéœ€ä¿®æ”¹ `base_url` å³å¯å°†ç°æœ‰ä»£ç åˆ‡æ¢åˆ° Groqã€‚

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
- âš¡ **ä¸šç•Œæœ€å¿«** - 800+ tokens/s æ¨ç†é€Ÿåº¦
- ğŸ”„ **OpenAI å…¼å®¹** - æ— ç¼è¿ç§»ç°æœ‰ä»£ç 
- ğŸ **è¶…é«˜é…é¢** - 14,400 è¯·æ±‚/å¤©
- ğŸ’° **å®Œå…¨å…è´¹** - æ— éšè—è´¹ç”¨
- ğŸš€ **ä½å»¶è¿Ÿ** - æ¯«ç§’çº§é¦– token å“åº”

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ææ¡ä»¶

**å¿…éœ€ï¼š**
- âœ… å·²åˆ›å»º API å¯†é’¥

è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒï¼š[Groq æ³¨å†ŒæŒ‡å—](/providers/groq#æ³¨å†Œå’Œè´¦å·)

### 5 åˆ†é’Ÿå¿«é€Ÿç¤ºä¾‹

**æ–¹æ³• 1ï¼šä½¿ç”¨ Groq SDKï¼ˆæ¨èï¼‰**

```bash {filename="Bash"}
pip install groq
```

```python {filename="Python"}
from groq import Groq

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = Groq(api_key="YOUR_API_KEY")

# å‘é€è¯·æ±‚
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "è§£é‡Šä¸€ä¸‹ Groq çš„ LPU æŠ€æœ¯",
        }
    ],
    model="llama-3.3-70b-versatile",
)

# æ‰“å°å“åº”
print(chat_completion.choices[0].message.content)
```

**æ–¹æ³• 2ï¼šä½¿ç”¨ OpenAI SDKï¼ˆå…¼å®¹æ¨¡å¼ï¼‰**

```python {filename="Python"}
from openai import OpenAI

# åªéœ€ä¿®æ”¹ base_url å’Œ api_key
client = OpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key="YOUR_API_KEY"
)

# å…¶ä½™ä»£ç ä¸ OpenAI å®Œå…¨ç›¸åŒ
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ]
)

print(response.choices[0].message.content)
```

---

## ğŸ¤– æ”¯æŒçš„æ¨¡å‹

### Llama ç³»åˆ—ï¼ˆæ¨èï¼‰

| æ¨¡å‹ ID | å‚æ•°é‡ | ä¸Šä¸‹æ–‡ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|---------|-------|--------|------|---------|
| `llama-3.3-70b-versatile` | 70B | 128K | âš¡âš¡âš¡ | é€šç”¨ä»»åŠ¡ï¼Œæœ€ä½³å¹³è¡¡ |
| `llama-3.1-70b-versatile` | 70B | 128K | âš¡âš¡âš¡ | å¤æ‚ä»»åŠ¡ |
| `llama-3.1-8b-instant` | 8B | 128K | âš¡âš¡âš¡âš¡ | å¿«é€Ÿå“åº” |

### å…¶ä»–æ¨¡å‹

| æ¨¡å‹ ID | å‚æ•°é‡ | ä¸Šä¸‹æ–‡ | ç‰¹ç‚¹ |
|---------|-------|--------|------|
| `mixtral-8x7b-32768` | 47B | 32K | Mistral æ··åˆä¸“å®¶ |
| `gemma2-9b-it` | 9B | 8K | Google å¼€æº |
| `deepseek-r1-distill-llama-70b` | 70B | 32K | æ¨ç†ä¸“å®¶ |

---

## ğŸ”¢ é…é¢å’Œé™åˆ¶

### API é…é¢

| é™åˆ¶ç±»å‹ | å…è´¹å±‚çº§ | è¯´æ˜ |
|---------|---------|------|
| **æ¯æ—¥è¯·æ±‚æ•°** | 14,400 requests | æ‰€æœ‰æ¨¡å‹å…±äº« |
| **æ¯åˆ†é’Ÿè¯·æ±‚æ•°** | 30 requests | æ‰€æœ‰æ¨¡å‹å…±äº« |
| **æ¯æ—¥ Tokens** | 20,000-1,000,000 | è§†æ¨¡å‹è€Œå®š |
| **æ¯åˆ†é’Ÿ Tokens** | 6,000 tokens | è¾“å…¥è¾“å‡ºæ€»å’Œ |
| **æœ€å¤§ä¸Šä¸‹æ–‡** | 128K tokens | Llama 3.x ç³»åˆ— |
| **æœ€å¤§è¾“å‡º** | 8,192 tokens | å•æ¬¡è¯·æ±‚ |

### âš ï¸ é‡è¦é™åˆ¶

1. **é…é¢å…±äº«ï¼š** æ‰€æœ‰æ¨¡å‹å…±äº«åŒä¸€è´¦æˆ·é…é¢
2. **é€Ÿç‡é™åˆ¶ï¼š** è¶…è¿‡é™åˆ¶è¿”å› 429 é”™è¯¯
3. **æ¯æ—¥é‡ç½®ï¼š** UTC æ—¶é—´å‡Œæ™¨ 00:00 é‡ç½®
4. **å¹¶å‘é™åˆ¶ï¼š** å»ºè®®ä¸è¶…è¿‡ 10 ä¸ªå¹¶å‘è¯·æ±‚

---

## ğŸ“– API ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€å¯¹è¯

```python {filename="Python"}
from groq import Groq

client = Groq(api_key="YOUR_API_KEY")

response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ]
)

print(response.choices[0].message.content)
```

### 2. æµå¼è¾“å‡º

```python {filename="Python"}
from groq import Groq

client = Groq(api_key="YOUR_API_KEY")

stream = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "user", "content": "å†™ä¸€ç¯‡å…³äºAIçš„æ–‡ç« "}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### 3. å¤šè½®å¯¹è¯

```python {filename="Python"}
from groq import Groq

client = Groq(api_key="YOUR_API_KEY")

messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹åŠ©æ‰‹"}
]

# ç¬¬ä¸€è½®
messages.append({"role": "user", "content": "å¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ"})
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=messages
)
messages.append({"role": "assistant", "content": response.choices[0].message.content})

# ç¬¬äºŒè½®ï¼ˆæ¨¡å‹ä¼šè®°ä½ä¸Šä¸‹æ–‡ï¼‰
messages.append({"role": "user", "content": "é‚£å†™å…¥å‘¢ï¼Ÿ"})
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=messages
)
print(response.choices[0].message.content)
```

### 4. è°ƒæ•´ç”Ÿæˆå‚æ•°

```python {filename="Python"}
from groq import Groq

client = Groq(api_key="YOUR_API_KEY")

response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "user", "content": "å†™ä¸€é¦–è¯—"}
    ],
    temperature=0.9,        # æé«˜åˆ›é€ æ€§
    max_tokens=1024,        # é™åˆ¶è¾“å‡ºé•¿åº¦
    top_p=0.95,            # æ ¸é‡‡æ ·
    frequency_penalty=0.5,  # é™ä½é‡å¤
    presence_penalty=0.5    # å¢åŠ è¯é¢˜å¤šæ ·æ€§
)

print(response.choices[0].message.content)
```

### 5. ä½¿ç”¨ cURL

```bash {filename="Bash"}
curl "https://api.groq.com/openai/v1/chat/completions" \
  -X POST \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "llama-3.3-70b-versatile",
    "messages": [
      {
        "role": "user",
        "content": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ Groq"
      }
    ]
  }'
```

### 6. Node.js ç¤ºä¾‹

```bash {filename="Bash"}
npm install groq-sdk
```

```javascript {filename="JavaScript"}
const Groq = require("groq-sdk");

const groq = new Groq({
  apiKey: "YOUR_API_KEY"
});

async function main() {
  const chatCompletion = await groq.chat.completions.create({
    messages: [
      { role: "user", content: "ä»‹ç»ä¸€ä¸‹ Groq çš„ LPU æŠ€æœ¯" }
    ],
    model: "llama-3.3-70b-versatile",
  });

  console.log(chatCompletion.choices[0].message.content);
}

main();
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

1. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹**
   ```python
   # å¿«é€Ÿä»»åŠ¡
   model = "llama-3.1-8b-instant"
   
   # å¹³è¡¡æ€§èƒ½
   model = "llama-3.3-70b-versatile"
   
   # æ•°å­¦/ä»£ç æ¨ç†
   model = "deepseek-r1-distill-llama-70b"
   ```

2. **å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•**
   ```python
   import time
   from groq import Groq, RateLimitError
   
   client = Groq(api_key="YOUR_API_KEY")
   
   def call_with_retry(messages, max_retries=3):
       for i in range(max_retries):
           try:
               return client.chat.completions.create(
                   model="llama-3.3-70b-versatile",
                   messages=messages
               )
           except RateLimitError:
               if i < max_retries - 1:
                   wait_time = 2 ** i
                   print(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                   time.sleep(wait_time)
               else:
                   raise
   ```

3. **ç›‘æ§é…é¢ä½¿ç”¨**
   ```python
   import logging
   
   logging.basicConfig(level=logging.INFO)
   logger = logging.getLogger(__name__)
   
   def track_usage(response):
       usage = response.usage
       logger.info(f"Tokensä½¿ç”¨: "
                   f"è¾“å…¥={usage.prompt_tokens}, "
                   f"è¾“å‡º={usage.completion_tokens}, "
                   f"æ€»è®¡={usage.total_tokens}")
       return response
   ```

4. **ä½¿ç”¨æµå¼è¾“å‡ºä¼˜åŒ–ä½“éªŒ**
   ```python
   def stream_chat(prompt):
       stream = client.chat.completions.create(
           model="llama-3.3-70b-versatile",
           messages=[{"role": "user", "content": prompt}],
           stream=True
       )
       
       full_response = ""
       for chunk in stream:
           if chunk.choices[0].delta.content:
               content = chunk.choices[0].delta.content
               print(content, end="", flush=True)
               full_response += content
       
       return full_response
   ```

5. **å®‰å…¨ç®¡ç† API å¯†é’¥**
   ```python
   import os
   from dotenv import load_dotenv
   
   # ä½¿ç”¨ç¯å¢ƒå˜é‡
   load_dotenv()
   api_key = os.getenv('GROQ_API_KEY')
   
   client = Groq(api_key=api_key)
   ```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### 1. 429 é”™è¯¯ï¼šRate Limit Exceeded

**åŸå› ï¼š** è¶…è¿‡é€Ÿç‡é™åˆ¶

**è§£å†³ï¼š**
```python {filename="Python"}
import time
from groq import RateLimitError

try:
    response = client.chat.completions.create(...)
except RateLimitError as e:
    print(f"é€Ÿç‡é™åˆ¶: {e}")
    print("ç­‰å¾…åé‡è¯•...")
    time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
    response = client.chat.completions.create(...)
```

### 2. 401 é”™è¯¯ï¼šInvalid API Key

**åŸå› ï¼š** API å¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ

**è§£å†³ï¼š**
1. æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®
2. åœ¨ Console é‡æ–°ç”Ÿæˆå¯†é’¥
3. ç¡®ä¿å¯†é’¥æ²¡æœ‰å¤šä½™çš„ç©ºæ ¼

### 3. ç½‘ç»œè¶…æ—¶

**è§£å†³ï¼š**
```python {filename="Python"}
from groq import Groq

client = Groq(
    api_key="YOUR_API_KEY",
    timeout=60.0  # è®¾ç½®60ç§’è¶…æ—¶
)
```

### 4. å¦‚ä½•æŸ¥çœ‹å‰©ä½™é…é¢ï¼Ÿ

**æ–¹æ³•ï¼š**
1. ç™»å½• https://console.groq.com
2. è®¿é—® "Usage" é¡µé¢
3. æŸ¥çœ‹å½“å‰é…é¢ä½¿ç”¨æƒ…å†µ

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†

```python {filename="Python"}
import asyncio
from groq import AsyncGroq

async def batch_process(questions):
    client = AsyncGroq(api_key="YOUR_API_KEY")
    
    tasks = []
    for q in questions:
        task = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": q}]
        )
        tasks.append(task)
    
    return await asyncio.gather(*tasks)

# ä½¿ç”¨
questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
responses = asyncio.run(batch_process(questions))
```

### 2. ç¼“å­˜ç»“æœ

```python {filename="Python"}
from functools import lru_cache
import hashlib
import json

def get_cache_key(messages):
    return hashlib.md5(
        json.dumps(messages, sort_keys=True).encode()
    ).hexdigest()

cache = {}

def cached_completion(messages):
    key = get_cache_key(messages)
    
    if key in cache:
        print("ä½¿ç”¨ç¼“å­˜ç»“æœ")
        return cache[key]
    
    response = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=messages
    )
    
    cache[key] = response
    return response
```

### 3. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```python {filename="Python"}
def select_model(task_complexity):
    if task_complexity == "simple":
        return "llama-3.1-8b-instant"  # æœ€å¿«
    elif task_complexity == "medium":
        return "llama-3.3-70b-versatile"  # å¹³è¡¡
    elif task_complexity == "complex":
        return "deepseek-r1-distill-llama-70b"  # æ¨ç†
```

---

## ğŸ“š ç›¸å…³èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Groq å®Œæ•´ä»‹ç»](/providers/groq)
- [Playground ä½¿ç”¨æŒ‡å—](/services/chatbot/groq)
- [API å®˜æ–¹æ–‡æ¡£](https://console.groq.com/docs)

### SDK å’Œå·¥å…·
- [Python SDK](https://github.com/groq/groq-python)
- [Node.js SDK](https://github.com/groq/groq-typescript)
- [API å‚è€ƒ](https://console.groq.com/docs/api-reference)

### ç¤ºä¾‹ä»£ç 
- [ç¤ºä¾‹åº“](https://github.com/groq/groq-api-cookbook)
- [ç¤¾åŒºç¤ºä¾‹](https://discord.gg/groq)

---

## ğŸŒŸ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šå®æ—¶èŠå¤©æœºå™¨äºº

```python {filename="Python"}
import streamlit as st
from groq import Groq

client = Groq(api_key="YOUR_API_KEY")

st.title("Groq è¶…å¿«èŠå¤©æœºå™¨äºº")

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # ç”Ÿæˆå›å¤
    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=st.session_state.messages,
            stream=True
        )
        
        response = st.write_stream(
            (chunk.choices[0].delta.content for chunk in stream 
             if chunk.choices[0].delta.content)
        )
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
    st.session_state.messages.append({"role": "assistant", "content": response})
```

### æ¡ˆä¾‹ 2ï¼šä»£ç åŠ©æ‰‹

```python {filename="Python"}
from groq import Groq

client = Groq(api_key="YOUR_API_KEY")

def code_assistant(language, task):
    response = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[
            {
                "role": "system",
                "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{language}ç¼–ç¨‹åŠ©æ‰‹ã€‚"
                           f"æä¾›æ¸…æ™°çš„ä»£ç ç¤ºä¾‹å’Œè§£é‡Šã€‚"
            },
            {
                "role": "user",
                "content": task
            }
        ],
        temperature=0.2  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„ä»£ç 
    )
    
    return response.choices[0].message.content

# ä½¿ç”¨
result = code_assistant("Python", "å®ç°ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾ç®—æ³•")
print(result)
```

---

**æœåŠ¡æä¾›è€…ï¼š** [Groq](/providers/groq)
