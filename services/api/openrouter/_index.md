---
title: OpenRouter - API
weight: 3
comments: true
sidebar:
  open: true
---

## ğŸ“‹ æœåŠ¡ä¿¡æ¯

**æä¾›è€…ï¼š** [OpenRouter](/providers/openrouter)  
**æœåŠ¡ç±»å‹ï¼š** API æœåŠ¡  
**API ç«¯ç‚¹ï¼š** [https://openrouter.ai/api/v1](https://openrouter.ai/api/v1)  
**å…è´¹ç±»å‹ï¼š** æ°¸ä¹…å…è´¹ï¼ˆæœ‰ä½¿ç”¨é™åˆ¶ï¼‰  
**API å…¼å®¹æ€§ï¼š** å®Œå…¨å…¼å®¹ OpenAI API

---

## ğŸ¯ æœåŠ¡ç®€ä»‹

OpenRouter API é€šè¿‡ç»Ÿä¸€çš„æ¥å£æä¾› 47+ ä¸ªå…è´¹ AI æ¨¡å‹ï¼Œå®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼ï¼Œè®©æ‚¨å¯ä»¥è½»æ¾åˆ‡æ¢å’Œæµ‹è¯•ä¸åŒçš„æ¨¡å‹ã€‚

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
- ğŸ¯ **ä¸€ç«™å¼è®¿é—®** - ä¸€ä¸ª API å¯†é’¥è®¿é—® 47+ æ¨¡å‹
- ğŸ”„ **OpenAI å…¼å®¹** - æ— ç¼è¿ç§»ç°æœ‰ä»£ç 
- ğŸ **å¤§é‡å…è´¹æ¨¡å‹** - åŒ…æ‹¬ Llama 3.1 405B ç­‰é¡¶çº§æ¨¡å‹
- ğŸš€ **æ™ºèƒ½è·¯ç”±** - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æä¾›è€…
- ğŸ’° **çµæ´»å®šä»·** - å…è´¹æ¨¡å‹æ°¸ä¸æ‰£è´¹

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ææ¡ä»¶

**å¿…éœ€ï¼š**
- âœ… å·²åˆ›å»º API å¯†é’¥

è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒï¼š[OpenRouter æ³¨å†ŒæŒ‡å—](/providers/openrouter#æ³¨å†Œå’Œè´¦å·)

### 5 åˆ†é’Ÿå¿«é€Ÿç¤ºä¾‹

**ä½¿ç”¨ OpenAI SDKï¼ˆæ¨èï¼‰**

```python {filename="Python"}
from openai import OpenAI

# é…ç½® OpenRouter
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="YOUR_OPENROUTER_API_KEY",
)

# è°ƒç”¨å…è´¹æ¨¡å‹
response = client.chat.completions.create(
    model="meta-llama/llama-3.3-70b-instruct:free",
    messages=[
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ OpenRouter"}
    ]
)

print(response.choices[0].message.content)
```

---

## ğŸ¤– å…è´¹æ¨¡å‹åˆ—è¡¨

### å¦‚ä½•è¯†åˆ«å…è´¹æ¨¡å‹

å…è´¹æ¨¡å‹çš„ model ID ä»¥ `:free` ç»“å°¾ï¼š
- âœ… `meta-llama/llama-3.3-70b-instruct:free`
- âŒ `meta-llama/llama-3.3-70b-instruct`ï¼ˆä»˜è´¹ï¼‰

### é¡¶çº§å…è´¹æ¨¡å‹

| Model ID | å‚æ•°é‡ | ä¸Šä¸‹æ–‡ | ç‰¹ç‚¹ |
|----------|-------|--------|------|
| `meta-llama/llama-3.3-70b-instruct:free` | 70B | 128K | Meta æœ€æ–° |
| `meta-llama/llama-3.1-405b-instruct:free` | 405B | 128K | è¶…å¤§å‚æ•° |
| `deepseek/deepseek-r1-0528:free` | - | 64K | æ¨ç†ä¸“å®¶ |
| `qwen/qwen-3-235b-a22b:free` | 235B | 128K | é˜¿é‡Œæ——èˆ° |
| `mistralai/mistral-small-3.1-24b:free` | 24B | 32K | Mistral é«˜æ•ˆ |

### è½»é‡å¿«é€Ÿæ¨¡å‹

| Model ID | å‚æ•°é‡ | ä¸Šä¸‹æ–‡ | ç‰¹ç‚¹ |
|----------|-------|--------|------|
| `meta-llama/llama-3.2-3b-instruct:free` | 3B | 128K | è½»é‡å¿«é€Ÿ |
| `google/gemma-3-4b-instruct:free` | 4B | 8K | å°è€Œå¼ºå¤§ |
| `mistralai/mistral-7b-instruct:free` | 7B | 32K | ç»å…¸é«˜æ•ˆ |

### ä»£ç ä¸“ç”¨æ¨¡å‹

| Model ID | ç‰¹ç‚¹ |
|----------|------|
| `mistralai/devstral-2512:free` | Mistral ä»£ç ä¸“å®¶ |
| `qwen/qwen-3-coder:free` | é€šä¹‰åƒé—®ä»£ç ç‰ˆ |

---

## ğŸ”¢ é…é¢å’Œé™åˆ¶

### API é…é¢

| é™åˆ¶ç±»å‹ | åŸºç¡€å±‚ | å‡çº§å±‚ï¼ˆ$10ï¼‰ |
|---------|--------|-------------|
| **æ¯æ—¥è¯·æ±‚æ•°** | 50 requests | 1,000 requests |
| **æ¯åˆ†é’Ÿè¯·æ±‚æ•°** | 20 requests | 20 requests |
| **å…è´¹æ¨¡å‹** | 47+ | 47+ |
| **è´¹ç”¨** | $0 | $10 ä¸€æ¬¡æ€§ |

### âš ï¸ é‡è¦é™åˆ¶

1. **æ¨¡å‹æ ‡è¯†ï¼š** å¿…é¡»ä½¿ç”¨ `:free` åç¼€çš„æ¨¡å‹ ID
2. **é…é¢å…±äº«ï¼š** æ‰€æœ‰å…è´¹æ¨¡å‹å…±äº«é…é¢
3. **é€Ÿç‡é™åˆ¶ï¼š** è¶…è¿‡é™åˆ¶è¿”å› 429 é”™è¯¯
4. **ä½™é¢ä¿æŠ¤ï¼š** å¯è®¾ç½®ä¿¡ç”¨é™é¢ä¸º $0 é¿å…è¯¯ç”¨ä»˜è´¹æ¨¡å‹

---

## ğŸ“– API ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€å¯¹è¯

**Pythonï¼ˆOpenAI SDKï¼‰ï¼š**

```python {filename="Python"}
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="YOUR_OPENROUTER_API_KEY"
)

response = client.chat.completions.create(
    model="meta-llama/llama-3.3-70b-instruct:free",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ]
)

print(response.choices[0].message.content)
```

### 2. æµå¼è¾“å‡º

```python {filename="Python"}
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="YOUR_OPENROUTER_API_KEY"
)

stream = client.chat.completions.create(
    model="meta-llama/llama-3.3-70b-instruct:free",
    messages=[
        {"role": "user", "content": "å†™ä¸€ç¯‡å…³äºAIçš„æ–‡ç« "}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 3. ä½¿ç”¨ requests åº“

```python {filename="Python"}
import requests

url = "https://openrouter.ai/api/v1/chat/completions"
headers = {
    "Authorization": "Bearer YOUR_OPENROUTER_API_KEY",
    "Content-Type": "application/json"
}
data = {
    "model": "meta-llama/llama-3.3-70b-instruct:free",
    "messages": [
        {"role": "user", "content": "ä½ å¥½"}
    ]
}

response = requests.post(url, headers=headers, json=data)
print(response.json()["choices"][0]["message"]["content"])
```

### 4. cURL ç¤ºä¾‹

```bash {filename="Bash"}
curl https://openrouter.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_OPENROUTER_API_KEY" \
  -d '{
    "model": "meta-llama/llama-3.3-70b-instruct:free",
    "messages": [
      {
        "role": "user",
        "content": "ä»‹ç»ä¸€ä¸‹ OpenRouter"
      }
    ]
  }'
```

### 5. Node.js ç¤ºä¾‹

```javascript {filename="JavaScript"}
const fetch = require('node-fetch');

async function chat() {
    const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Authorization': 'Bearer YOUR_OPENROUTER_API_KEY',
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            model: 'meta-llama/llama-3.3-70b-instruct:free',
            messages: [
                { role: 'user', content: 'ä½ å¥½' }
            ]
        })
    });
    
    const data = await response.json();
    console.log(data.choices[0].message.content);
}

chat();
```

### 6. å¤šè½®å¯¹è¯

```python {filename="Python"}
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="YOUR_OPENROUTER_API_KEY"
)

messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹åŠ©æ‰‹"}
]

# ç¬¬ä¸€è½®
messages.append({"role": "user", "content": "å¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ"})
response = client.chat.completions.create(
    model="meta-llama/llama-3.3-70b-instruct:free",
    messages=messages
)
messages.append({"role": "assistant", "content": response.choices[0].message.content})

# ç¬¬äºŒè½®
messages.append({"role": "user", "content": "é‚£å†™å…¥å‘¢ï¼Ÿ"})
response = client.chat.completions.create(
    model="meta-llama/llama-3.3-70b-instruct:free",
    messages=messages
)
print(response.choices[0].message.content)
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

1. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹**
   ```python
   # å¿«é€Ÿä»»åŠ¡
   model = "meta-llama/llama-3.2-3b-instruct:free"
   
   # é€šç”¨ä»»åŠ¡
   model = "meta-llama/llama-3.3-70b-instruct:free"
   
   # å¤æ‚ä»»åŠ¡
   model = "meta-llama/llama-3.1-405b-instruct:free"
   
   # ä¸­æ–‡ä»»åŠ¡
   model = "qwen/qwen-3-235b-a22b:free"
   
   # ä»£ç ä»»åŠ¡
   model = "mistralai/devstral-2512:free"
   ```

2. **è®¾ç½®ä½™é¢é™åˆ¶**
   ```python
   # åœ¨ OpenRouter æ§åˆ¶å°è®¾ç½®
   # Settings â†’ Credits â†’ Monthly Limit â†’ $0
   # è¿™æ ·å¯ä»¥é˜²æ­¢è¯¯ç”¨ä»˜è´¹æ¨¡å‹
   ```

3. **å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•**
   ```python
   import time
   from openai import OpenAI, RateLimitError
   
   client = OpenAI(
       base_url="https://openrouter.ai/api/v1",
       api_key="YOUR_OPENROUTER_API_KEY"
   )
   
   def call_with_retry(messages, max_retries=3):
       for i in range(max_retries):
           try:
               return client.chat.completions.create(
                   model="meta-llama/llama-3.3-70b-instruct:free",
                   messages=messages
               )
           except RateLimitError:
               if i < max_retries - 1:
                   wait_time = 2 ** i
                   print(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                   time.sleep(wait_time)
               else:
                   raise
   ```

4. **ç›‘æ§ä½¿ç”¨æƒ…å†µ**
   ```python
   # è®¿é—® https://openrouter.ai/activity
   # æŸ¥çœ‹è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬
   ```

5. **å®‰å…¨ç®¡ç† API å¯†é’¥**
   ```python
   import os
   from dotenv import load_dotenv
   
   load_dotenv()
   api_key = os.getenv('OPENROUTER_API_KEY')
   
   client = OpenAI(
       base_url="https://openrouter.ai/api/v1",
       api_key=api_key
   )
   ```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### 1. å¦‚ä½•æŸ¥çœ‹æ‰€æœ‰å…è´¹æ¨¡å‹ï¼Ÿ

**æ–¹æ³•ï¼š**
- è®¿é—®ï¼šhttps://openrouter.ai/models?free=true
- æˆ–ä½¿ç”¨ APIï¼š
  ```python
  response = requests.get(
      "https://openrouter.ai/api/v1/models",
      headers={"Authorization": f"Bearer {api_key}"}
  )
  free_models = [m for m in response.json()["data"] if ":free" in m["id"]]
  ```

### 2. 429 é”™è¯¯ï¼šRate Limit Exceeded

**åŸå› ï¼š** è¶…è¿‡æ¯æ—¥æˆ–æ¯åˆ†é’Ÿé™åˆ¶

**è§£å†³ï¼š**
- ç­‰å¾…é…é¢é‡ç½®ï¼ˆæ¯æ—¥UTC 00:00ï¼‰
- å‡çº§åˆ° 1000 requests/dayï¼ˆå……å€¼ $10ï¼‰
- å®ç°è¯·æ±‚ç¼“å­˜å’Œé‡è¯•é€»è¾‘

### 3. ä¸ºä»€ä¹ˆä¼šæ‰£è´¹ï¼Ÿ

**åŸå› ï¼š** ä½¿ç”¨äº†æ²¡æœ‰ `:free` åç¼€çš„æ¨¡å‹

**é¢„é˜²ï¼š**
```python {filename="Python"}
def ensure_free_model(model_id):
    if not model_id.endswith(":free"):
        raise ValueError(f"æ¨¡å‹ {model_id} ä¸æ˜¯å…è´¹æ¨¡å‹ï¼")
    return model_id

# ä½¿ç”¨
model = ensure_free_model("meta-llama/llama-3.3-70b-instruct:free")
```

### 4. å¦‚ä½•åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼Ÿ

**æ–¹æ³•ï¼š**
```python {filename="Python"}
# åªéœ€ä¿®æ”¹ model å‚æ•°
models_to_try = [
    "meta-llama/llama-3.3-70b-instruct:free",
    "meta-llama/llama-3.1-405b-instruct:free",
    "qwen/qwen-3-235b-a22b:free"
]

for model in models_to_try:
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": "æµ‹è¯•"}]
    )
    print(f"{model}: {response.choices[0].message.content[:50]}")
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python {filename="Python"}
def select_model(task_type, complexity):
    if task_type == "code":
        return "mistralai/devstral-2512:free"
    elif task_type == "chinese":
        return "qwen/qwen-3-235b-a22b:free"
    elif complexity == "high":
        return "meta-llama/llama-3.1-405b-instruct:free"
    else:
        return "meta-llama/llama-3.3-70b-instruct:free"
```

### 2. ç»“æœç¼“å­˜

```python {filename="Python"}
from functools import lru_cache
import hashlib
import json

cache = {}

def get_cache_key(model, messages):
    content = json.dumps({"model": model, "messages": messages}, sort_keys=True)
    return hashlib.md5(content.encode()).hexdigest()

def cached_completion(model, messages):
    key = get_cache_key(model, messages)
    
    if key in cache:
        print("ä½¿ç”¨ç¼“å­˜ç»“æœ")
        return cache[key]
    
    response = client.chat.completions.create(
        model=model,
        messages=messages
    )
    
    cache[key] = response
    return response
```

### 3. æ‰¹é‡å¤„ç†

```python {filename="Python"}
import asyncio
from openai import AsyncOpenAI

async def batch_process(questions):
    client = AsyncOpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key="YOUR_OPENROUTER_API_KEY"
    )
    
    tasks = []
    for q in questions:
        task = client.chat.completions.create(
            model="meta-llama/llama-3.3-70b-instruct:free",
            messages=[{"role": "user", "content": q}]
        )
        tasks.append(task)
    
    return await asyncio.gather(*tasks)

# ä½¿ç”¨
questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
responses = asyncio.run(batch_process(questions))
```

---

## ğŸ“š ç›¸å…³èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [OpenRouter å®Œæ•´ä»‹ç»](/providers/openrouter)
- [Playground ä½¿ç”¨æŒ‡å—](/services/chatbot/openrouter)
- [API å®˜æ–¹æ–‡æ¡£](https://openrouter.ai/docs)

### å·¥å…·å’Œèµ„æº
- [å…è´¹æ¨¡å‹åˆ—è¡¨](https://openrouter.ai/models?free=true)
- [API å‚è€ƒ](https://openrouter.ai/docs/api-reference)
- [ä½¿ç”¨ç»Ÿè®¡](https://openrouter.ai/activity)
- [å®šä»·è¯´æ˜](https://openrouter.ai/docs/limits)

---

## ğŸŒŸ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šå¤šæ¨¡å‹å¯¹æ¯”å·¥å…·

```python {filename="Python"}
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="YOUR_OPENROUTER_API_KEY"
)

def compare_models(prompt, models):
    results = {}
    for model in models:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        results[model] = response.choices[0].message.content
    return results

# ä½¿ç”¨
models = [
    "meta-llama/llama-3.3-70b-instruct:free",
    "qwen/qwen-3-235b-a22b:free",
    "mistralai/mistral-small-3.1-24b:free"
]

results = compare_models("è§£é‡Šé‡å­è®¡ç®—", models)
for model, answer in results.items():
    print(f"\n{model}:\n{answer[:200]}...")
```

### æ¡ˆä¾‹ 2ï¼šæ™ºèƒ½æ¨¡å‹è·¯ç”±

```python {filename="Python"}
def smart_router(task, content):
    if "ä»£ç " in task or "ç¼–ç¨‹" in task:
        model = "mistralai/devstral-2512:free"
    elif "ä¸­æ–‡" in task or len(content) > len(content.encode('utf-8')) * 0.7:
        model = "qwen/qwen-3-235b-a22b:free"
    elif len(content) > 5000:
        model = "meta-llama/llama-3.1-405b-instruct:free"
    else:
        model = "meta-llama/llama-3.3-70b-instruct:free"
    
    return model

# ä½¿ç”¨
task = "å†™ä¸€ä¸ªPythonå‡½æ•°"
model = smart_router(task, task)
print(f"é€‰æ‹©æ¨¡å‹: {model}")
```

---

**æœåŠ¡æä¾›è€…ï¼š** [OpenRouter](/providers/openrouter)
